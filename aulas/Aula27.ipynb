{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IifuDlIpv2cD"
   },
   "source": [
    "# Mapas de difusão\n",
    "\n",
    "* Grafos\n",
    "* Matriz de adjacência e grau\n",
    "* Grafo Laplaciano\n",
    "* Corte normalizado\n",
    "* Matriz de Afinidade\n",
    "* Passeios aleatórios\n",
    "* Mapas de Difusão\n",
    "* Exemplos e comparação com PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UuwCSDERv2cF"
   },
   "source": [
    "## Representação matricial de um grafo\n",
    "\n",
    "Um grafo $G = (V, E)$ pode ser reprensentado por uma matriz $A$, $n \\times n$ com $n = |V|$, da seguinte forma:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "A_{ij} = 1 \\quad\\text{se}\\quad (i, j) \\in E \\\\\n",
    "\\quad 0 \\quad\\text{c.c.}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gSV69Gfcv2cG"
   },
   "source": [
    "A matriz $A$ é chamada de matriz de adjacência pois indica se dois vértices são adjacentes.\n",
    "Note que se o grafo é não direcionado a matriz $A$ é simétrica pois tanto $(i, j)$ quanto $(j, i)$ pertencem a $E$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "91ntRKI1v2cH"
   },
   "source": [
    "Em seguida podemos definir a matriz diagonal grau $D$ cuja entrada $D_{ii}$ é o grau do vértice $i$.\n",
    "\n",
    "\\begin{equation}\n",
    "D_{ii} = \\sum_{j} A_{ij} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RzUy4sZRv2cJ"
   },
   "source": [
    "Se o grafo $G$ possui arestas com pesos, substituimos a primeira equação por $A_{ij} = w_{ij}$, onde $w_{ij}$ é o peso da aresta ou zero caso não exista a aresta $(i,j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_NYUGD0sv2cK"
   },
   "source": [
    "## Laplaciano de um grafo\n",
    "\n",
    "O laplaciano de um grafo é definido como\n",
    "\n",
    "\\begin{equation}\n",
    "L = D - A\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JysVy-cPv2cL"
   },
   "source": [
    "A matriz $L$ recebe este nome pois ela pode ser interpretada como um caso do operador discreto de Laplace, suponha que $\\phi$ descreva a distribuição de calor no grafo, temos\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{d\\phi}{dt} = -k \\sum_j A_{ij}(\\phi_i - \\phi_j)\n",
    "$$\n",
    "\n",
    "fazendo algumas transformações temos\n",
    "\n",
    "$$\n",
    "\\frac{d\\phi}{dt} = -k (D - A)\\phi = -kL\\phi\n",
    "$$\n",
    "\n",
    "\n",
    "Como a equação toma a mesma forma da equação do calor $\\frac{du}{dt} = \\alpha\\nabla^2u$, com $-L$ sendo o operador de laplace, chamamos este de laplaciano do grafo.\n",
    "\n",
    "A partir do laplaciano de um grafo podemos extrair informações interessantes sobre a estrutura do grafo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iIxBE4MQv2cM"
   },
   "source": [
    "## Cortes de um grafo\n",
    "\n",
    "Um corte em um grafo é uma partição dos vertices em conjuntos disjuntos. Em geral um corte busca minimizar alguma propriedade dada as partições.\n",
    "\n",
    "Por exemplo, o corte minimo de um grafo minimiza o peso das arestas entre as partições.\n",
    "\n",
    "Um tipo de corte que é interessante para nos, é o corte normalizado que busca tambem minimizar as arestas entre as partições mas de certa forma que a partições tenham uma forte conexão internamente.\n",
    "\n",
    "Dadas duas partições $A$ e $B$ um corte normalizado minimiza a seguinte equação\n",
    "\n",
    "$$\n",
    "\\text{ncut}(A, B) = \\frac{w(A, B)}{w(A, V)} + \\frac{w(A, B)}{w(B, V)}\n",
    "$$\n",
    "\n",
    "onde \n",
    "$$\n",
    "w(X, Y) = \\sum_{i \\in X, j \\in Y} w_{ij}\n",
    "$$.\n",
    "\n",
    "Note que a ideia do corte fica muito mais clara se escrevemos $w(A, V) = w(A, A) + w(A, B)$ e $w(B, V) = w(B, B) + w(A, B)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vb1t6mbmv2cN"
   },
   "source": [
    "Pode se mostrar que a solução do corte é dada por\n",
    "\n",
    "$$\n",
    "\\min \\text{ncut}(A, B) = \\frac{\\mathbf{y}^\\top L \\mathbf{y}}{\\mathbf{y}^\\top D \\mathbf{y}}\n",
    "$$\n",
    "\n",
    "sujeito a\n",
    "\n",
    "$$\n",
    "y_i \\in \\left\\{1, -b\\right\\}, \\quad b = cte\n",
    "$$\n",
    "$$\n",
    "\\mathbf{y}^\\top D \\mathbb{1} = 0\n",
    "$$\n",
    "\n",
    "Este problema é NP-hard mas se relaxarmos a solução $\\mathbf{y}$ para ter valores nos reais, temos um problema de autovalor-autovetor\n",
    "\n",
    "$$\n",
    "L \\mathbf{y} = \\lambda D \\mathbf{y}\n",
    "$$\n",
    "\n",
    "\n",
    "Por fim, podemos usar o sinal de $y_i$ para determinar se o vértice $i$ pertence a partição $A$ ou $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jjx6b7Wdv2cN"
   },
   "source": [
    "## Matriz de afinidade\n",
    "\n",
    "Até o momento descrevemos como representar um grafo matricialmente, o que é um corte e mostramos a relação do corte normalizado com o laplaciano de um grafo.\n",
    "\n",
    "A questão importante no entanto, que não foi respondida ainda, é como usamos essas ferramentas para aplicar machine learning.\n",
    "\n",
    "\n",
    "O conceito que faz a ponte entre grafos e machine learning é considerar que um conjunto de dados é um grafo onde as amostras são os vértices e o peso das arestas entre os vértices é dado por alguma medida de afinidade (ou similaridade) entre as amostras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kejl12ZPv2cO"
   },
   "source": [
    "Definimos então uma função kernel $k$, simétrica e positiva definida\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{y}) = exp\\left(\\frac{-\\lVert\\mathbf{x} - \\mathbf{y}\\rVert ^2}{\\epsilon^2}\\right)\n",
    "$$\n",
    "\n",
    "onde o parametro $\\epsilon$ define o tamanho da vizinhança de afinidade.\n",
    "\n",
    "Logo temos a matriz de afinidade para um conjunto de dados $X$\n",
    "\n",
    "$$\n",
    "A_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j), \\quad \\mathbf{x}_i, \\mathbf{x}_j \\in X\n",
    "$$\n",
    "\n",
    "Note que esta matriz é bem semelhante a matriz de adjacência. Com isso já é possível aplicar o corte normalizado aos dados, obtendo um método (não supervisionado) de clusterização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0iO58gzv2cP"
   },
   "source": [
    "## Passeios aleatórios\n",
    "\n",
    "Suponha agora que desejamos considerar afinidades de maior ordem, isto é, afinidades indiretas. Por exemplo, se $x$ tem afinidade com $y$ e $y$ tem afinidade com $z$ então $x$ tem afinidade com $z$\n",
    "\n",
    "Podemos pensar no nosso grafo como uma cadeia de Markov e que com o tempo $t$ as probabilidades de transição (relativas à afinidade) difundem a vértices vizinhos (com grande afinidade).\n",
    "\n",
    "Assim chegamos a matriz de Markov $P$ \n",
    "$$\n",
    "P = D^{-1}A\n",
    "$$\n",
    "\n",
    "e $P^t$ nos dá as probabilidades de transição para um determinado tempo de difusão $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FoSioEtMv2cP"
   },
   "source": [
    "## Mapas de difusão\n",
    "\n",
    "O método dos mapas de difusão se baseia na construção da matriz $P$ e suas potências, dai o plural \"mapas\" pois para cada $t$ temos um mapa diferente dos dados em escalas diferentes.\n",
    "\n",
    "Podemos ver o método como uma \"mudança de base\" dos dados de forma que a nova base é definida a partir dos proprios dados. \n",
    "Note que \"mudança de base\" está entre aspas pois esta transformação não é uma mudança de base no sentido classico.\n",
    "No entanto ao escrever mudança de base remetemos ao uso do método PCA, por isso será feita uma comparação entre ambos os métodos posteriormente.\n",
    "\n",
    "Ao olhar para as linhas da matriz $P$ podemos ver esta como cuma transformação dos nosso dados, ao invés de uma amostra ser representada por valores absolutos, $\\mathbf{x}_i$, representamos ele por $p_i$ sua afinidade (ou relação) com o resto dos dados, que dizer $i$-ésima linha da matriz $P$.\n",
    "\n",
    "Observe que essa representação é bem interessante pois ignora fatores como rotações e escala dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWP0Tr9Ov2cQ"
   },
   "source": [
    "Introduzimos então as distâncias de difusão\n",
    "\n",
    "$$\n",
    "D(\\mathbf{x}_i, \\mathbf{x}_j)^2 = \\lVert p_i - p_j\\rVert ^2\n",
    "$$\n",
    "\n",
    "mas note que podemos escrever o lado esquerdo em termos dos autovalores $\\lambda_i$ e autovetores $\\mathbf{v}_i$ de $P$\n",
    "\n",
    "$$\n",
    "D(\\mathbf{x}_i, \\mathbf{x}_j)^2 = \\lVert p_i - p_j\\rVert ^2 = \\sum_k \\lambda_k \\lVert v_k(i) - v_k(j)\\rVert ^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-xnrzhcv2cR"
   },
   "source": [
    "E assim podemos definir uma imersão dos dados que aproxima a distância de difusão truncando a equação acima aos primeiros $n$ autovalores de maior norma\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i \\mapsto \\left\\{\\lambda_1 v_1(i), \\cdots, \\lambda_n v_n(i)\\right\\}\n",
    "$$\n",
    "\n",
    "Ao escolher $n$ menor que a dimensão dos dados obtemos uma redução de dimensionalidade ótima no sentido de que esta é a melhor aproximação da distancia de difusão original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pRqHh-nuv2cR"
   },
   "source": [
    "### Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_0EHxAYKv2cS"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import normalize\n",
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "class DiffusionMaps:\n",
    "    def __init__(self, metric = 'euclidean'):\n",
    "        self.metric = metric;\n",
    "\n",
    "    def fit(self, X, eps):\n",
    "        self.X = X.copy()\n",
    "        self.eps = eps;\n",
    "        D = pairwise_distances(X, metric=self.metric)\n",
    "        E = np.exp(-(D * D) / (eps**2))\n",
    "        P = normalize(E, axis=1, norm='l1')\n",
    "        e, V = np.linalg.eig(P)\n",
    "        idx = np.argsort(e.real)\n",
    "        e = e.real[idx[::-1]]\n",
    "        V = V.real[:, idx[::-1]]\n",
    "        self.e = e\n",
    "        self.V = V\n",
    "        return e, V\n",
    "    \n",
    "    def transform(self, X):\n",
    "        #D = cdist(X, self.X, 'sqeuclidean')\n",
    "        D = pairwise_distances(X, self.X, metric=self.metric)\n",
    "        E = np.exp(-(D * D) / (self.eps**2))\n",
    "        P = normalize(E, axis=1, norm='l1')\n",
    "        return P.dot(self.V).dot(np.diag(1.0 / self.e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQtmSTg-r4yg"
   },
   "source": [
    "### Testando o mapas de difusão com o conjunto de digitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NYdM4CAQr4yh"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import math\n",
    "\n",
    "data = load_digits()\n",
    "X = data.data\n",
    "lab = data.target\n",
    "\n",
    "dm = DiffusionMaps()\n",
    "\n",
    "e, V = dm.fit(X, 12)\n",
    "\n",
    "#idx = np.argsort(V[:, 1])\n",
    "idx = range(0, len(V[:,1]))\n",
    "x = V[idx, 1]\n",
    "y = V[idx, 2]\n",
    "z = V[idx, 3]\n",
    "w = V[idx, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75Es-MUEr4yj",
    "outputId": "698a521b-feef-4194-b928-5c1854d1a38c"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y, c=lab[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VMq_d-i-r4ym",
    "outputId": "147d9e9b-2cc6-4764-c20c-d481340727c4"
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "def make_scatter(x, y, lab):\n",
    "    classes = np.unique(lab)\n",
    "    #viridis = cm.get_cmap('viridis', len(classes))\n",
    "    cmap = cm.get_cmap('tab10', len(classes))\n",
    "    plt.set_cmap(cmap)\n",
    "    names = map(str, classes)\n",
    "    plots = []\n",
    "    for c in classes:\n",
    "        idx = lab == c\n",
    "        color = np.array(cmap(c)).reshape(1, 4)\n",
    "        p = plt.scatter(x[idx], y[idx], c=color);\n",
    "        plots.append(p)\n",
    "    plt.legend(plots, names, loc='best', frameon=False)\n",
    "\n",
    "\n",
    "idx = np.argsort(x)\n",
    "i = np.array(range(0, len(x)))\n",
    "colors = lab[idx].astype(int)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "make_scatter(x, y, lab); plt.xlabel(\"X\");plt.ylabel(\"Y\")\n",
    "plt.subplot(1, 2, 2)\n",
    "make_scatter(z, w, lab); plt.xlabel(\"Z\");plt.ylabel(\"W\")\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "make_scatter(i, x[idx], colors); plt.xlabel(\"Index\");plt.ylabel(\"X\")\n",
    "plt.subplot(1, 2, 2)\n",
    "make_scatter(i, y[idx], colors); plt.xlabel(\"Index\");plt.ylabel(\"Y\")\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "make_scatter(i, z[idx], colors); plt.xlabel(\"Index\");plt.ylabel(\"Z\")\n",
    "plt.subplot(1, 2, 2)\n",
    "make_scatter(i, w[idx], colors); plt.xlabel(\"Index\");plt.ylabel(\"W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUKdIGZpr4yo"
   },
   "source": [
    "### Comparando com o PCA\n",
    "\n",
    "Nesta seção vamos comparar visualmente a transformação dos dados assim como foi feito com o mapa de difusão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_DYwuX3Rr4yp",
    "outputId": "f3092b49-f6f7-4ded-86c5-048792b7bb4f"
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "pca = decomposition.PCA(n_components=10)\n",
    "pca.fit(X)\n",
    "U = pca.transform(X)\n",
    "\n",
    "x = U[:, 0]\n",
    "y = U[:, 1]\n",
    "z = U[:, 2]\n",
    "w = U[:, 3]\n",
    "plt.scatter(x, y, c=lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WEwGCB9Ar4yr",
    "outputId": "22df7025-77e3-4299-dfa4-d384115f31c9"
   },
   "outputs": [],
   "source": [
    "idx = np.argsort(x)\n",
    "i = np.array(range(0, len(x)))\n",
    "colors = lab[idx].astype(int)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "make_scatter(x, y, lab); plt.xlabel(\"X\");plt.ylabel(\"Y\")\n",
    "plt.subplot(1, 2, 2)\n",
    "make_scatter(z, w, lab); plt.xlabel(\"Z\");plt.ylabel(\"W\")\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "make_scatter(i, x[idx], colors); plt.xlabel(\"Index\");plt.ylabel(\"X\")\n",
    "plt.subplot(1, 2, 2)\n",
    "make_scatter(i, y[idx], colors); plt.xlabel(\"Index\");plt.ylabel(\"Y\")\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "make_scatter(i, z[idx], colors); plt.xlabel(\"Index\");plt.ylabel(\"Z\")\n",
    "plt.subplot(1, 2, 2)\n",
    "make_scatter(i, w[idx], colors); plt.xlabel(\"Index\");plt.ylabel(\"W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VY-LCiByr4yt"
   },
   "source": [
    "### Comparativo classificador usando a transformação do DM e do PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k-A1upA8r4yu"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,lab,random_state = 0)\n",
    "\n",
    "\n",
    "pca = decomposition.PCA(n_components=10)\n",
    "pca.fit(X_train)\n",
    "U = pca.transform(X_train)\n",
    "\n",
    "dm = DiffusionMaps()\n",
    "e, V = dm.fit(X_train, 12)\n",
    "\n",
    "ncomp = 4;\n",
    "At = pca.transform(X_train)[:, range(0, ncomp)]\n",
    "A = pca.transform(X_test)[:, range(0, ncomp)]\n",
    "# note que no mapa de difusão nao usamos o primeiro autovetor V[:,0]\n",
    "# pois este é o autovetor 1 constante para todas as entradas\n",
    "# como as linhas da matriz P somam 1 entao P * [1] = 1 * [1]\n",
    "Bt = dm.transform(X_train)[:, range(1, ncomp+1)]\n",
    "B = dm.transform(X_test)[:, range(1, ncomp+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzAzV50Sr4yw",
    "outputId": "418c70a8-d56f-4b21-e86d-f2e0594b31d7"
   },
   "outputs": [],
   "source": [
    "#plot de ambas as transformaçoes tando dos dados de teste quanto de treino\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.scatter(U[:, 0], U[:, 1], c=y_train)\n",
    "plt.scatter(A[:, 0], A[:, 1], c=y_test, marker='^')\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.scatter(V[:, 1], V[:, 2], c=y_train)\n",
    "plt.scatter(B[:, 0], B[:, 1], c=y_test, marker='^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21QoZOaXr4yy",
    "outputId": "30905470-e7bb-4b4c-ae09-5dafd58de6b9"
   },
   "outputs": [],
   "source": [
    "#plor dos dados de teste somente\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.scatter(A[:, 0], A[:, 1], c=y_test, marker='^')\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.scatter(B[:, 0], B[:, 1], c=y_test, marker='^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9MKdgIrPr4y1",
    "outputId": "56ff9c66-4d88-423c-c1fa-1a4aeecb2ee7"
   },
   "outputs": [],
   "source": [
    "# comparativo do classificador knn usando o dm e o pca\n",
    "\n",
    "c1 = KNeighborsClassifier(n_neighbors = 5)\n",
    "c2 = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "c1.fit(At, y_train)\n",
    "c2.fit(Bt, y_train)\n",
    "print('PCA train: {}'.format(c1.score(At, y_train)))\n",
    "print('PCA test: {}'.format(c1.score(A, y_test)))\n",
    "\n",
    "print('DM train: {}'.format(c2.score(Bt, y_train)))\n",
    "print('DM test: {}'.format(c2.score(B, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Shq-hMSLr4y3"
   },
   "source": [
    "### Considerções sobre ambos os métodos\n",
    "\n",
    "* PCA é um método linear enquanto o DM é não linear\n",
    "* A complexidade do PCA depende da dimensão dos dados enquanto o DM depende do número de amostras\n",
    "* Para obter bons resultados no DM é necessário uma boa escolha do parâmetro epsilon\n",
    "\n",
    "\n",
    "Devido a essas características o PCA é eficiente quando a dimensão dos dados é pequena ou média, podendo ser aplicado rapidamente a um comjunto de dados largo.\n",
    "\n",
    "Por outro lado o DM é mais indicado quando o conjunto de dados não é muito grande, ou pode ser subamostrado, mas em contrapartida a dimensão dos dados pode ser muito elevada sem afetar sua eficiência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L6wGc7udr4y4"
   },
   "source": [
    "## Mais Exemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VugHmuaKr4y4"
   },
   "outputs": [],
   "source": [
    "n = 200\n",
    "t = np.linspace(0, 2*np.pi, n);\n",
    "x = np.cos(t) * 5 + np.random.random(t.shape);\n",
    "y = np.sin(t) * 5 + np.random.random(t.shape);\n",
    "\n",
    "x2 = np.cos(t) + np.random.random(t.shape);\n",
    "y2 = np.sin(t) + np.random.random(t.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q1Sl9OFJr4y6",
    "outputId": "70dac31f-2819-4a34-e2b6-c705b0da47b4"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y, c='b')\n",
    "plt.scatter(x2, y2, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EAs6wTOr4y8",
    "outputId": "e577ced4-a2c1-46f3-a301-59ccf8b43f86"
   },
   "outputs": [],
   "source": [
    "X = np.concatenate([x, x2])\n",
    "Y = np.concatenate([y, y2])\n",
    "data = np.reshape(np.concatenate([X, Y]), (2, -1)).T\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cV_wVrOZr4y_",
    "outputId": "00d2fd96-cc3a-4f36-fac2-ccd43f179749"
   },
   "outputs": [],
   "source": [
    "dm = DiffusionMaps()\n",
    "\n",
    "dm.fit(data, 1)\n",
    "V = dm.transform(data)[:, range(1, 3)]\n",
    "\n",
    "plt.scatter(V[range(0, n), 0], V[range(0, n), 1], c='b')\n",
    "plt.scatter(V[range(n, 2*n), 0], V[range(n, 2*n), 1], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "av9qS1UOr4zB"
   },
   "source": [
    "### Perfis sintéticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tSz47Nr89vJG"
   },
   "outputs": [],
   "source": [
    "#função para teste, o parametro 'a' representa as diferentes curvas\n",
    "def f(x, a):\n",
    "    b = np.sqrt(4 - a*a)\n",
    "    return 10 - 20 * a * np.exp(-a * x) * np.sin(b * x) / b + 10 * a * np.exp(-a * x) * np.cos(b * x)\n",
    "\n",
    "x = np.linspace(0.08, 8, 100);\n",
    "y5 = f(x, 0.5)\n",
    "y7 = f(x, 0.7)\n",
    "y9 = f(x, 0.9)\n",
    "\n",
    "plt.plot(x, y5)\n",
    "plt.plot(x, y7)\n",
    "plt.plot(x, y9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_samples(y1, y2, sigma, n1, n2):\n",
    "    rows = n1 + n2\n",
    "    cols = len(y1)\n",
    "    data = np.zeros((rows, cols))\n",
    "    for i in range(0, n1):\n",
    "        data[i, :] = y1 + np.random.normal(0, sigma, (1, cols))\n",
    "    for i in range(n1, n1 + n2):\n",
    "        data[i, :] = y2 + np.random.normal(0, sigma, (1, cols))\n",
    "        \n",
    "    return data\n",
    "\n",
    "n1 = 90\n",
    "n2 = 10\n",
    "sigma = 0.5\n",
    "data = make_samples(y5, y7, sigma, n1, n2)\n",
    "\n",
    "plt.plot(x, data[0, :])\n",
    "plt.plot(x, data[-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DiffusionMaps()\n",
    "\n",
    "dm.fit(data, 5)\n",
    "V = dm.transform(data)[:, range(1, 3)]\n",
    "\n",
    "plt.scatter(V[range(0, n1), 0], V[range(0, n1), 1], c='b')\n",
    "plt.scatter(V[range(n1, n1+n2), 0], V[range(n1, n1+n2), 1], c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "\n",
    "pca.fit(data)\n",
    "V = pca.transform(data)\n",
    "\n",
    "plt.scatter(V[range(0, n1), 0], V[range(0, n1), 1], c='b')\n",
    "plt.scatter(V[range(n1, n1+n2), 0], V[range(n1, n1+n2), 1], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = pd.read_csv('spam.csv')\n",
    "data['y'] = data['target'] == 'spam'\n",
    "\n",
    "X = data['text']\n",
    "y = data['y']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "cv = CountVectorizer(stop_words='english').fit(X_train)\n",
    "tv = TfidfVectorizer(stop_words='english').fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A = cv.transform(X_train)\n",
    "B = cv.transform(X_test)\n",
    "\n",
    "dm = DiffusionMaps()\n",
    "\n",
    "eps = 50\n",
    "e, V = dm.fit(A.astype(float), eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Va = dm.transform(A.astype(float))[:, range(1, 21)]\n",
    "Vb = dm.transform(B.astype(float))[:, range(1, 21)]\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(Va[:, 0], Va[:, 1], c=y_train)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(Va[:, 2], Va[:, 3], c=y_train)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(Va[:, 4], Va[:, 5], c=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf = MultinomialNB().fit(A, y_train)\n",
    "pred = clf.predict(B)\n",
    "\n",
    "print('MNB', roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "n = 4\n",
    "c1.fit(Va[:, range(0,n)], y_train)\n",
    "pred = c1.predict(Vb[:, range(0, n)])\n",
    "\n",
    "print('DM test: {}'.format(roc_auc_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "with open('newsgroups', 'rb') as f:\n",
    "    newsgroup_data = pickle.load(f)\n",
    "    \n",
    "cv = CountVectorizer(min_df=20, max_df=0.2, stop_words='english', \n",
    "                       token_pattern='(?u)\\\\b\\\\w{3,}\\\\b')\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "\n",
    "X = cv.fit_transform(newsgroup_data)\n",
    "Xtf = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DiffusionMaps(metric = 'cosine')\n",
    "\n",
    "eps = 0.5 #400\n",
    "e, V = dm.fit(Xtf.astype(float), eps)\n",
    "Vn = V[:, range(1, 7)]\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(Vn[:, 0], Vn[:, 1])\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(Vn[:, 2], Vn[:, 3])\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(Vn[:, 4], Vn[:, 5])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "aula_diff_maps.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
